
Can correlations be localized in typical tasks?

The first simulation is for two tasks.  In performing simulations, we must set the sample sizes, ground truth relations among the tasks, trial noise and true individual variation.  The former two were set by the preceding analysis.  For all of our simulations, we used $I=`r typical$I`$ people and $L=`r typical$L`$ replicates per condition.  We think these are good choices to emulate designs where many individuals are going to run in several inhibition tasks.  For tasks with two conditions, there are 40,000 observations per task.  In a typical battery with $J=10$ tasks, the total number of observations is 400,000, which is quite large.  Hence, our choices seem appropriate to typical large-scale individual-difference studies with experimental tasks.  

Using the typical sample sizes discussed above, each hypothetical data set consisted of 80,000 observations (200 people $\times$ 2 tasks $\times$ 2 conditions $\times$ 100 replicates per condition).    The last input to the simulations is the true correlation across the two tasks.  This value were varied through levels of .2, .5, and .8.  For each of these levels, 100 data sets were simulated and analyzed. 



```{r eval=sim2LKJFlag | sim2WishFlag}
observed=function(dat){
         mrt=tapply(dat$rt,list(dat$sub,dat$task,dat$cond),mean)
        effect=mrt[,,2]-mrt[,,1]
        return(cor(effect))}

set.seed(345)
R=100*3
M=300
burn=100
pop.cor=rep(c(.2,.5,.8),each=R/3)
cor=matrix(nrow=M-burn,ncol=R)
spearman.cor=true.cor=observed.cor=modWish.cor=modLKJ.cor=1:R
if (sim2LKJFlag) stanLkjM <- stan_model(model_code = stanLkjC)

for (r in 1:R){
  out=sim2(typical,pop.cor[r])
  dat=out$dat
  if (sim2WishFlag){ 
    mod=genModWish2(dat,M=M)
    for (m in 1:(M-burn)){
      Sigma=solve(mod$B[m+burn,,])
      cor[m,r]=cov2cor(Sigma)[1,2]}}
  if (sim2LKJFlag){ 
    stanData <- list(y=dat$rt, 
                   n=length(dat$sub), I=typical$I,J=2, 
             sub=dat$sub, cond=dat$cond, task=dat$task)
    samples <- sampling(stanLkjM, data=stanData, iter=600, chains=1,warmup=200)
    Omega <- extract(samples)$Omega
    OmegaS[r,] = Omega[,1,2]}      
  spearman.cor[r]=spearman(dat)[1,2]
  observed.cor[r]=observed(dat)[1,2]
  true.cor[r]=cor(out$t.theta)[1,2]}

results=cbind(pop.cor,
              true.cor,
              observed.cor,
              spearman.cor)

write.table(round(results,3),file="twoTaskSimResults",row.names=F,quote=F)

if (sim2LKJFlag) save(file="twoLKJ.Rdata",OmegaS)
if (sim2WishFlag) save(file="twoWish.Rdata",cor) 
```


```{r recov2,fig.asp=1.0,fig.cap="Revocery of correlations from two tasks.  A: Boxplots of recovered correlations from sample correlations, Spearman's correction, and the hierarchical model.  B-D: Posterior 95% credible intervals for the model-recovered correlations for true correlations of .2, .5, and .8, respectively."}
resa=read.table('twoTaskSimResults',head=T)
load("twoWish.Rdata")
res=cbind(resa,apply(cor,2,mean))
myCol=c(rgb(.8,0,0,.3),rgb(0,.4,0,.3),rgb(0,0,.8,.3))
myColSat=c(rgb(1,0,0),rgb(0,.5,0),rgb(0,0,1))
layout(matrix(nrow=2,ncol=3,byrow = T,c(1,1,1,2,3,4)))
par(mar=c(4,4,1,1),mgp=c(2,.7,0),cex=1.0)
small=res[,1]==.2
med=res[,1]==.5
large=res[,1]==.8
#boxplot(cbind(res[small,3:5],res[med,3:5],res[large,3:5]),
#              col=myColSat,axes=F,ylim=c(-.5,max(res)),
#        at=c(1:3,5:7,9:11),cex=.7,ylab="Correlation Estimate")
x=rep(c(1,5,9,2,6,10,3,7,11),each=100)
y=res[,3:5]
plot(jitter(x),as.matrix(y),pch=19,col=rep(myCol,each=300,3),axes=F,
     ylab="Posterior Means",xlab="")
axis(2)
axis(1,at=c(1,3),lab=c("",""))
mtext(expression(rho ~ "=" ~ ".2"), side = 1, at = 2, line = .7)
axis(1,at=c(5, 7),lab=c("",""))
mtext(expression(rho ~ "=" ~ ".5"), side = 1, at = 6, line = .7)
axis(1,at=c(9, 11),lab=c("",""))
mtext(expression(rho ~ "=" ~ ".8"), side = 1, at = 10, line = .7)
segments(.5,.2,3.5,.2,lwd=2)
segments(4.5,.5,7.5,.5,lwd=2)
segments(8.5,.8,11.5,.8,lwd=2)
mtext(side=3,"A.",adj=.1,cex=1.1,line=0)
par(xpd=NA)
legend(9.5,.2,fill=myColSat,legend=c("Sample","Spearman","Model"), bty = "n", cex = .9)
par(xpd=F)

 myPch=c(15,17,19)
 
# matplot(res[,2],res[,3:5],pch=myPch,col=myCol,
#         xlab="Correlation among True Individual Scores",
#         ylab="Estimated Correlation",xlim=c(0,1),ylim=c(-.55,1.2))
# abline(h=0,lty=2)
# abline(0,1)
# abline(1,0,lty=2)
# legend(.75,.03,legend=c("Naive","Corrected","Model"),pch=myPch,col=myColSat,bg="white")


makeStats=function(cors){
  mn=apply(cors,1,mean)
  lo=apply(cors,1,quantile,p=.025)
  hi=apply(cors,1,quantile,p=.975)
  o=order(mn)
  return(cbind(lo[o],mn[o],hi[o]))
}

w=1:3
par(mar=c(4,1.5,1,1))
a=makeStats(t(cor[,res[,1]==.2]))
w[1]=mean(a[,1]<.2 & a[,3]>.2)
R=dim(a)[1]
plot(a[,2],1:R,ylab="",xlim=c(-1,1),pch=20,cex=.8, col = "white",
     xlab="Posterior Correlation",axes=F)
arrows(code=3,angle=90,a[,1],1:R,a[,3],1:R,length=.02,col='grey')
points(a[,2],1:R,pch=20,cex=.8, col = adjustcolor(1, .7))
axis(1, cex.axis = .75)
mtext(side=3,"B.",adj=0,cex=1.1,line= -1)
abline(v=.2)

a=makeStats(t(cor[,res[,1]==.5]))
w[2]=mean(a[,1]<.5 & a[,3]>.5)
R=dim(a)[1]
plot(a[,2],1:R,ylab="",xlim=c(-1,1),pch=20,cex=.8, col = "white",
     xlab="Posterior Correlation",axes=F)
arrows(code=3,angle=90,a[,1],1:R,a[,3],1:R,length=.02,col="grey")
points(a[,2],1:R,pch=20,cex=.8, col = adjustcolor(1, .7))
axis(1, cex.axis = .75)
abline(v=.5)
mtext(side=3,"C.",adj=0,cex=1.1,line=-1)


a=makeStats(t(cor[,res[,1]==.8]))
w[3]=mean(a[,1]<.8 & a[,3]>.8)
R=dim(a)[1]
plot(a[,2],1:R,ylab="",xlim=c(-1,1),pch=19,cex=.8, col = "white",
     xlab="Posterior Correlation",axes=F)
arrows(code=3,angle=90,a[,1],1:R,a[,3],1:R,length=.02,col='grey')
points(a[,2],1:R,pch=20,cex=.8, col = adjustcolor(1, .7))
axis(1, cex.axis = .75)
abline(v=.8)
mtext(side=3,"D.",adj=0,cex=1.1,line=-1)
```

```{r}
rmse=function(true,est) sqrt(mean((true-est)^2))
```

Figure \ref{fig:recov2}A shows the results.  The correlations from participant-by-task sample means are shown in red, and are called here "sample correlations."  As expected, these correlations suffer a large degree of attenuation from trial noise.  Correlation estimates from Spearman's correction are shown in green.  These values are better centered though some of the corrected values are greater than 1.0.  The correlation estimates from the hierarchical model are shown in blue. 

Overall, the correlation estimates from Spearman's correction and the hierarchical model have less bias than sample correlations.  Yet, the estimates are quite variable.  For example, consider correlations when the population value is .2.  The model estimates range from `r round(min(res[small,5]),2)` to `r round(max(res[small,5]),2)` and miss the target with a RMSE of `r round(rmse(res[small,5],res[small,1]),2)`.  Spearman corrected estimates are a slightly better and have an RMSE for this case of `r round(rmse(res[small,4],res[small,1]),2)`.   Overall though, this variability is quite high especially given the large number of observations.  The correlations are not well localizedm and we would not have confidence in substantive conclusions with it.

Figure \ref{fig:recov2}A shows only posterior mean estimates.  Yet, in the Bayesian approach, the target is not just the posterior mean, but the entirety of the posterior distribution.  Figure \ref{fig:recov2}B-D shows the posterior 95% credible intervals for all runs with true correlations of .2, .5, and .8, respectively.  There are two noteworthy trends.  First, the 95% credible intervals tend to contain the true value on `r round(mean(w)*100)`% of the simulation runs.  This means that the posterior variability is relatively well calibrated and provides reasonably accurate information on the uncertainty in the correlation.  Second, there is a fair amount of uncertainty meaning that the analyst knows that correlations have not been well localized.  With the Bayesian model-based estimates, at least we know how uncertain we are in localizing true correlations.  With sample correlation and with the Spearman correction, we have no such knowledge.
